by Jaafar El Komati - Wednesday, 1 October 2025, 9:12 AM

Murthy, your post really resonated with me, especially the way ethical concerns were linked to sociological concerns. I think the one regarding deepfakes is really pressing. We've seen how fast manipulated content spreads during elections and wars, and with AI now capable of producing realistic "evidence," the very nature of truth is open to attack (Chesney & Citron, 2019).

Your description of copyright as unsettled ground also holds true. The "fair use" question of AI training data is hot and courts are only just beginning to figure out what sort of compensation might be paid to creators (Hutson, 2023). It is one where policy will be likely to lag behind technology unless governments intervene quickly.

I also appreciated your insistence on collaboration. Regulating AI is not something that can be entrusted to the technology sector solely. As you said, civil society has to be brought into the equation. Indeed, some researchers argue that it is as vital to create digital literacy among the population as to enhance detection capabilities—because an educated population is more able to resist manipulation (Wardle, 2020).

Your entry leaves me wondering: what do you believe would most likely be the first realistic action—tighter copyright legislation, better detection tech, or education?

 

References

Chesney, R. & Citron, D.K. (2019). Deep fakes: A looming challenge for privacy, democracy, and national security. California Law Review, 107(6), pp. 1753–1820.
Hutson, M. (2023). Who owns AI-generated art? Nature, 621, 22–24.
Wardle, C. (2020). Understanding information disorder. Journal of Applied Journalism & Media Studies, 9(1), pp. 13–26.
