by Jaafar El Komati - Wednesday, 1 October 2025, 9:13 AM

Martina, your post was standing out because you used to discuss environmental dimensions - a concern that often looks at prejudice or misinformation debate. The fact that training a large NLP model can be equal to the lifetime emissions of many cars (Strubell et al., 2019). It shows AI morality not only as a social issue but also as a climate.

I also got your point about accountability compelling. The "black box" problem is not just technical; This reduces faith. If users cannot understand why the AI ​​system takes a decision, even the appropriate results may still feel illegitimate (Burrell, 2016). It makes sense to connect it with governance, as we are looking at initial experiments with AI regulation in the AI ​​Act of the European Union, although critics argue that it still struggles with enforcement and global scope (Veale & Borgesius, 2021).

Your reflection on dependence on AI was actually a hit at home as well. It is attractive to tilt on tools such as chatgpt for every task, but as you mentioned, it risk reducing human creativity. I think a combination of human input with AI aid-Shneiderman (2020) says "human-focused AI"-we should be balanced that we need.

Your closing call seems significant for the associate governance. Without shared standards, stability and accountability are risk patchwork solutions.


References

Burrell, J. (2016). How the machine ‘thinks’: Understanding opacity in machine learning algorithms. Big Data & Society, 3(1), 1–12.

Strubell, E., Ganesh, A., & McCallum, A. (2019). Energy and policy considerations for deep learning in NLP. Proceedings of ACL, 3645–3650.

Veale, M., & Borgesius, F. Z. (2021). Demystifying the draft EU Artificial Intelligence Act. Computer Law Review International, 22(4), 97–112.

Shneiderman, B. (2020). Human-centered AI. Oxford University Press.
